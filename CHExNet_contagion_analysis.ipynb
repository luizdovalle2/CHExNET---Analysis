{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c854e6",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1587fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def get_t(d, time_id):\n",
    "    # get first time_bin after a publication\n",
    "    if pd.isna(d):\n",
    "        return None\n",
    "    for k, v in time_id.items():\n",
    "        if d < v:\n",
    "            return k\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "df_authority = pd.read_parquet(\"data/authority_file_cac_alma.parquet\")\n",
    "df_authority = df_authority.drop_duplicates(subset=\"final_id\")\n",
    "CHExNet = pickle.load(open(\"data/CHExNet.pkl\", 'rb'))\n",
    "time_id = pickle.load(open(\"data/time_id_final.pkl\", 'rb'))\n",
    "m_AUJ = CHExNet['layer_1']\n",
    "m_BJ = CHExNet['layer_2']\n",
    "\n",
    "# get first time_bin after a publication\n",
    "df_authority[\"tid_from\"] = df_authority.first_polish_pub.apply(lambda x: get_t(x, time_id))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57c9579",
   "metadata": {},
   "source": [
    "## Calculate exporusre per time bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dce974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# Per-time-slice features for each layer\n",
    "dfs_auj, dfs_bj = [], []\n",
    "\n",
    "# Union of time keys across AUJ and BJ (unordered; sort(ts) if needed)\n",
    "ts = list(set(m_AUJ.keys()) | set(m_BJ.keys()))\n",
    "\n",
    "for t in ts:\n",
    "\n",
    "    # --- AUJ ---\n",
    "    auj_v = m_AUJ.get(t)\n",
    "    if auj_v:\n",
    "        W = auj_v[\"matrix\"]                 # sparse adjacency/exposure matrix\n",
    "        ids = list(auj_v[\"ids_pos_mat\"])    # node ids in matrix row/col order\n",
    "        cur_time = auj_v[\"time\"]            # snapshot date\n",
    "\n",
    "        deg = np.asarray(W.getnnz(axis=1)).ravel()  # row nnz = degree proxy\n",
    "\n",
    "        # adopter indicator: 1 if first_polish_pub < cur_time else 0\n",
    "        z = (\n",
    "            df_authority[df_authority.final_id.isin(ids)]\n",
    "            .first_polish_pub\n",
    "            .apply(lambda x: x < cur_time if isinstance(x, date) else False)\n",
    "            .to_numpy()\n",
    "            .astype(\"int8\")\n",
    "        )\n",
    "\n",
    "        x = W @ z                            # exposure to adopters (neighbor sum)\n",
    "        dfs_auj.append(pd.DataFrame({\"person_id\": ids, \"deg1\": deg, \"x1\": x, \"t\": t,\n",
    "                                     \"combined\": [(i, t) for i in ids]}))\n",
    "\n",
    "    # --- BJ ---\n",
    "    bj_v = m_BJ.get(t)\n",
    "    if bj_v:\n",
    "        W = bj_v[\"matrix\"]\n",
    "        ids = list(bj_v[\"ids_pos_mat\"])\n",
    "        cur_time = bj_v[\"time\"]\n",
    "\n",
    "        deg = np.asarray(W.getnnz(axis=1)).ravel()\n",
    "        z = (\n",
    "            df_authority[df_authority.final_id.isin(ids)]\n",
    "            .first_polish_pub\n",
    "            .apply(lambda x: x < cur_time if isinstance(x, date) else False)\n",
    "            .to_numpy()\n",
    "            .astype(\"int8\")\n",
    "        )\n",
    "\n",
    "        x = W @ z\n",
    "        dfs_bj.append(pd.DataFrame({\"person_id\": ids, \"deg2\": deg, \"x2\": x, \"t\": t,\n",
    "                                    \"combined\": [(i, t) for i in ids]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b17a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = pd.concat(dfs_auj, ignore_index=True)   # AUJ features\n",
    "df_b = pd.concat(dfs_bj,  ignore_index=True)   # BJ features\n",
    "\n",
    "df_expo = df_a.merge(df_b, how=\"outer\", on=[\"person_id\", \"t\"])  # join layers\n",
    "df_expo = df_expo.fillna(0).copy()                              # missing -> 0\n",
    "\n",
    "df_expo = df_expo.merge(                                       # add adoption timing\n",
    "    df_authority[[\"final_id\", \"first_polish_pub\", \"tid_from\"]],\n",
    "    left_on=\"person_id\", right_on=\"final_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "df_expo[\"y\"] = (df_expo[\"t\"] == df_expo[\"tid_from\"]).astype(int)  # event at tid_from\n",
    "df_py = df_expo.loc[df_expo[\"tid_from\"].isna() | (df_expo[\"t\"] <= df_expo[\"tid_from\"])].copy()  # risk set, exclude pairs after adoption\n",
    "\n",
    "#calculate century and filter 16, 17 and 18\n",
    "df_py[\"century\"] = df_py.t.apply( lambda t: time_id[t].year//100 +1)\n",
    "df_py = df_py[(df_py[\"century\"]>15) & (df_py[\"century\"]<19)]\n",
    "\n",
    "df_py[\"any1\"] = (df_py[\"x1\"] > 0).astype(int)                         # any AUJ exposure\n",
    "df_py[\"any2\"] = (df_py[\"x2\"] > 0).astype(int)                         # any BJ exposure\n",
    "df_py[\"any_or\"] = (df_py[\"any1\"].eq(1) | df_py[\"any2\"].eq(1)).astype(int)  # any exposure (either)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88075d46",
   "metadata": {},
   "source": [
    "## Generate table for exposure-count sparsity and any-exposure prevalence by layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85188fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exposure_row(x: pd.Series) -> dict:\n",
    "    \"\"\"Percent in {0,1,>=2} and percent exposed (>=1).\"\"\"\n",
    "    s = x.fillna(0)\n",
    "    n = len(s)\n",
    "    return {\n",
    "        \"% x=0\":   100.0 * (s.eq(0).sum() / n),\n",
    "        \"% x=1\":   100.0 * (s.eq(1).sum() / n),\n",
    "        \"% x>=2\":  100.0 * ((s.ge(2)).sum() / n),\n",
    "        \"% exposed\": 100.0 * ((s.ge(1)).sum() / n),\n",
    "    }\n",
    "\n",
    "# layer rows\n",
    "tab = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"Layer 1\": exposure_row(df_py[\"x1\"]),\n",
    "        \"Layer 2\": exposure_row(df_py[\"x2\"]),\n",
    "    },\n",
    "    orient=\"index\",\n",
    ")\n",
    "\n",
    "# either / both (any exposure indicators)\n",
    "any1 = df_py[\"x1\"].fillna(0).gt(0)\n",
    "any2 = df_py[\"x2\"].fillna(0).gt(0)\n",
    "\n",
    "tab.loc[\"Either layer\", [\"% exposed\"]] = 100.0 * (any1 | any2).mean()\n",
    "tab.loc[\"Both layers\",  [\"% exposed\"]] = 100.0 * (any1 & any2).mean()\n",
    "\n",
    "tab = tab[[\"% x=0\", \"% x=1\", \"% x>=2\", \"% exposed\"]].round(2)\n",
    "\n",
    "tab.to_csv(\"exposure_count_sparsity.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af67d0b1",
   "metadata": {},
   "source": [
    "## Fit contagion models and create results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f237c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2\n",
    "from firthmodels.adapters.statsmodels import FirthLogit\n",
    "\n",
    "# -------------------------\n",
    "# 1) Specify models\n",
    "# -------------------------\n",
    "models = {\n",
    "    \"M0\": \"y ~ C(century)\",\n",
    "    \"M1\": \"y ~ C(century) + any1\",\n",
    "    \"M2\": \"y ~ C(century) + any2\",\n",
    "    \"M3\": \"y ~ C(century) + any1 + any2\",\n",
    "    \"M4\": \"y ~ C(century) + any_or\",\n",
    "    \"M5\": \"y ~ C(century) + any1 + any2 + any1:any2\",\n",
    "}\n",
    "key_terms = [\"any1\", \"any2\", \"any_or\", \"any1:any2\"]\n",
    "\n",
    "# -------------------------\n",
    "# 2) Fit Firth models\n",
    "# -------------------------\n",
    "res = {name: FirthLogit.from_formula(formula, df_py).fit()\n",
    "       for name, formula in models.items()}\n",
    "\n",
    "def safe_get(obj, attr, default=np.nan):\n",
    "    v = getattr(obj, attr, None)\n",
    "    return default if v is None else float(v)\n",
    "\n",
    "# -------------------------\n",
    "# 3) Manual AIC/BIC + LR helper\n",
    "# -------------------------\n",
    "def manual_aic_bic(llf, k, n):\n",
    "    # AIC = -2*llf + 2*k ; BIC = -2*llf + log(n)*k \n",
    "    aic = -2*llf + 2*k\n",
    "    bic = -2*llf + np.log(n)*k\n",
    "    return float(aic), float(bic)\n",
    "\n",
    "def lr_test(full_res, base_res):\n",
    "    if not (hasattr(full_res, \"llf\") and hasattr(base_res, \"llf\")):\n",
    "        return (np.nan, 0, np.nan)\n",
    "    lr_stat = 2 * (float(full_res.llf) - float(base_res.llf))\n",
    "    df_diff = int(len(np.asarray(full_res.params)) - len(np.asarray(base_res.params)))\n",
    "    pval = float(chi2.sf(lr_stat, df_diff))\n",
    "    return float(lr_stat), df_diff, pval\n",
    "\n",
    "# -------------------------\n",
    "# 4) Build Table 4 with OR [95% CI] and manually-computed AIC/BIC\n",
    "# -------------------------\n",
    "m0 = res[\"M0\"]\n",
    "rows = []\n",
    "\n",
    "for name, r in res.items():\n",
    "    # names for parameters (adapter should have exog_names)\n",
    "    if hasattr(r, \"model\") and hasattr(r.model, \"exog_names\"):\n",
    "        names = list(r.model.exog_names)\n",
    "    else:\n",
    "        names = [f\"x{i}\" for i in range(len(np.asarray(r.params)))]\n",
    "\n",
    "    params = pd.Series(np.asarray(r.params), index=names, name=\"beta\")\n",
    "    p_two  = pd.Series(np.asarray(getattr(r, \"pvalues\", np.full(len(names), np.nan))), index=names, name=\"p_two\")\n",
    "\n",
    "    ci_arr = np.asarray(r.conf_int())\n",
    "    ci_b = pd.DataFrame(ci_arr, index=names, columns=[\"beta_low\", \"beta_high\"])\n",
    "\n",
    "    llf = safe_get(r, \"llf\")\n",
    "    k = int(len(params))  # number of estimated parameters (incl intercept + dummies)\n",
    "    n = int(getattr(r, \"nobs\", len(df_py)))\n",
    "\n",
    "    AIC = np.nan\n",
    "    BIC = np.nan\n",
    "    if np.isfinite(llf):\n",
    "        AIC, BIC = manual_aic_bic(llf, k, n)\n",
    "\n",
    "    row = {\n",
    "        \"model\": name,\n",
    "        \"formula\": models[name],\n",
    "        \"llf\": llf,\n",
    "        \"nobs\": n,\n",
    "        \"k_params\": k,\n",
    "        \"AIC\": AIC,\n",
    "        \"BIC\": BIC,\n",
    "    }\n",
    "\n",
    "    for term in key_terms:\n",
    "        col_prefix = f\"exp(beta_{term})\"\n",
    "        if term in params.index:\n",
    "            b = float(params[term])\n",
    "            OR = float(np.exp(b))\n",
    "            OR_low = float(np.exp(ci_b.loc[term, \"beta_low\"]))\n",
    "            OR_high = float(np.exp(ci_b.loc[term, \"beta_high\"]))\n",
    "            row[f\"{col_prefix}_CI\"] = f\"{OR:.2f} [{OR_low:.2f}, {OR_high:.2f}]\"\n",
    "            row[f\"p({term})\"] = float(p_two.get(term, np.nan))\n",
    "        else:\n",
    "            row[f\"{col_prefix}_CI\"] = \"\"\n",
    "            row[f\"p({term})\"] = np.nan\n",
    "\n",
    "    # LR vs M0 (if llf exists)\n",
    "    if name != \"M0\":\n",
    "        lr, df_diff, p_lr = lr_test(r, m0)\n",
    "        row[\"LR_vs_M0\"] = lr\n",
    "        row[\"df_diff_vs_M0\"] = df_diff\n",
    "        row[\"p_LR_vs_M0\"] = p_lr\n",
    "\n",
    "        # deltas vs M0 (only if AIC/BIC finite)\n",
    "        row[\"AIC_delta_vs_M0\"] = (AIC - manual_aic_bic(safe_get(m0, \"llf\"), len(np.asarray(m0.params)), int(getattr(m0, \"nobs\", len(df_py))))[0]\n",
    "                                  if np.isfinite(AIC) and np.isfinite(safe_get(m0, \"llf\")) else np.nan)\n",
    "        row[\"BIC_delta_vs_M0\"] = (BIC - manual_aic_bic(safe_get(m0, \"llf\"), len(np.asarray(m0.params)), int(getattr(m0, \"nobs\", len(df_py))))[1]\n",
    "                                  if np.isfinite(BIC) and np.isfinite(safe_get(m0, \"llf\")) else np.nan)\n",
    "    else:\n",
    "        row[\"LR_vs_M0\"] = np.nan\n",
    "        row[\"df_diff_vs_M0\"] = 0\n",
    "        row[\"p_LR_vs_M0\"] = np.nan\n",
    "        row[\"AIC_delta_vs_M0\"] = 0.0\n",
    "        row[\"BIC_delta_vs_M0\"] = 0.0\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "table4 = pd.DataFrame(rows).set_index(\"model\")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Pretty headers and final display (2 decimals)\n",
    "# -------------------------\n",
    "pretty = {\n",
    "    \"any1\": r\"$\\exp(\\hat\\beta_1)$\",\n",
    "    \"any2\": r\"$\\exp(\\hat\\beta_2)$\",\n",
    "    \"any1:any2\": r\"$\\exp(\\hat\\beta_3)$\",\n",
    "     \"any_or\": r\"$\\exp(\\hat\\beta_or)$\",\n",
    "    \n",
    "}\n",
    "rename_map = {}\n",
    "for term in key_terms:\n",
    "    rename_map[f\"exp(beta_{term})_CI\"] = pretty[term] + \" [95% CI]\"\n",
    "    rename_map[f\"p({term})\"] = f\"p({pretty[term]})\"\n",
    "table4 = table4.rename(columns=rename_map)\n",
    "\n",
    "# -------------------------\n",
    "# 6) Final display (2 decimals)  -- now includes any_or\n",
    "# -------------------------\n",
    "table4_display = table4[[\n",
    "    \"formula\", \"llf\", \"AIC\", \"BIC\",\n",
    "\n",
    "    pretty[\"any1\"] + \" [95% CI]\", f\"p({pretty['any1']})\",\n",
    "    pretty[\"any2\"] + \" [95% CI]\", f\"p({pretty['any2']})\",\n",
    "    pretty[\"any_or\"] + \" [95% CI]\", f\"p({pretty['any_or']})\",          # <-- add\n",
    "    pretty[\"any1:any2\"] + \" [95% CI]\", f\"p({pretty['any1:any2']})\",\n",
    "\n",
    "    \"LR_vs_M0\", \"df_diff_vs_M0\", \"p_LR_vs_M0\", \"AIC_delta_vs_M0\", \"BIC_delta_vs_M0\",\n",
    "]].copy()\n",
    "\n",
    "# round numeric model-fit columns to 2 decimals\n",
    "num_cols = [\"llf\", \"AIC\", \"BIC\", \"LR_vs_M0\", \"AIC_delta_vs_M0\", \"BIC_delta_vs_M0\"]\n",
    "table4_display[num_cols] = table4_display[num_cols].round(2)\n",
    "\n",
    "# format p-values to 2 decimals\n",
    "p_cols = [\n",
    "    f\"p({pretty['any1']})\",\n",
    "    f\"p({pretty['any2']})\",\n",
    "    f\"p({pretty['any_or']})\",            \n",
    "    f\"p({pretty['any1:any2']})\",\n",
    "    \"p_LR_vs_M0\"\n",
    "]\n",
    "table4_display[p_cols] = table4_display[p_cols].applymap(\n",
    "    lambda x: \"\" if pd.isna(x) else f\"{float(x):.2f}\"\n",
    ")\n",
    "\n",
    "table4_display.to_csv(\"table_contagion_firth.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b810be",
   "metadata": {},
   "source": [
    "## Generate crude hazard by decade plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b86249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator, FormatStrFormatter\n",
    "\n",
    "d = df_py.copy()\n",
    "\n",
    "# map t -> calendar year\n",
    "d[\"year\"] = d[\"t\"].map(lambda t: time_id[t].year)\n",
    "\n",
    "# decade start (1503 -> 1500, 1519 -> 1510, etc.)\n",
    "d[\"decade\"] = (d[\"year\"] // 10) * 10\n",
    "\n",
    "life_dec = (\n",
    "    d.groupby(\"decade\", as_index=False)\n",
    "     .agg(at_risk=(\"person_id\", \"count\"),\n",
    "          events=(\"y\", \"sum\"))\n",
    "     .sort_values(\"decade\")\n",
    ")\n",
    "\n",
    "life_dec[\"hazard\"] = life_dec[\"events\"] / life_dec[\"at_risk\"]\n",
    "\n",
    "# ---- plot ----\n",
    "x = life_dec[\"decade\"] + 5  # decade midpoint\n",
    "\n",
    "fig, ax_h = plt.subplots(figsize=(11, 4))\n",
    "\n",
    "# line = hazard (left axis)\n",
    "ax_h.plot(x, life_dec[\"hazard\"], color=\"black\", linewidth=2)\n",
    "ax_h.set_xlabel(\"Decade\")\n",
    "ax_h.set_ylabel(\"Crude hazard (events / at risk)\")\n",
    "\n",
    "# bars = counts (right axis)\n",
    "ax_e = ax_h.twinx()  # shares x-axis; ticks on the right \n",
    "ax_e.bar(x, life_dec[\"events\"], width=8, color=\"steelblue\", alpha=0.25, edgecolor=\"none\")\n",
    "ax_e.set_ylabel(\"First adoptions (count)\")\n",
    "\n",
    "# right axis: integers only (no fractions)\n",
    "ax_e.yaxis.set_major_locator(MaxNLocator(integer=True, min_n_ticks=1))  \n",
    "ax_e.yaxis.set_major_formatter(FormatStrFormatter('%.0f'))             \n",
    "\n",
    "# paper-friendly styling\n",
    "ax_h.set_xlim(life_dec[\"decade\"].min(), life_dec[\"decade\"].max() + 10)\n",
    "ax_h.spines[\"top\"].set_visible(False)\n",
    "ax_e.spines[\"top\"].set_visible(False)\n",
    "\n",
    "ax_h.set_title(\"Crude hazard by decade (with first-adoption counts)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"crude_hazard.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
